<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[获取mysql某个库里面，每张表里每个字段为空的数量，占这张表总数的比例]]></title>
    <url>%2F2018%2F03%2F15%2Fmysql-1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;公司购买了某金融公司的数据（存储介质mysql），里面有500多张表。产品经理在做产品时，需要分析500张表里的信息，挖取想要的数据。但很多表里的很多字段都是NULL，比如fund(基金)表里有个net_value(净值)字段，一共有100条数据，有99条数据这个字段为null,则这个字段可靠度不高，产品经理就不会使用这个字段。但500张表，几千个字段，不可能一个个去 where xxx= NULL把,需要脚本批量得出每个字段为空的占比，提高工作效率。下面就开始我的脚本研究心路历程(= =,希望听懂了，我为啥要写这个脚本) &emsp;&emsp;首先要感谢我的好基友DBA小圈圈的大力支持(脚本都是他写，哈哈)。接下来进入正题。要完成 ‘获取mysql某个库里面，每张表里每个字段为空的数量，占这张表总数的比例’这个问题，首先肯定要通过某种便捷方法得到所有的table 以及这个table下的所有filed。额，询问了我的基友，原来sql就是支持的，只是平时对于我这个开发而言都没用到而已。看sql：12SELECT TABLE_SCHEMA,TABLE_NAME,COLUMN_NAMEFROM information_schema.COLUMNS; 结果如下图：&emsp;&emsp;这个sql会把这个mysql实例下所有的schema里的table_name 和column_name一一枚举出来。通过上述sql，我就可以拼接出想要的sql:123SELECT concat('select count(', COLUMN_NAME, ')/count(*) from ', TABLE_SCHEMA, '.', TABLE_NAME)FROM information_schema.COLUMNS; 结果如下图：&emsp;&emsp;额，这样sql的粒度是按字段来，即同一个表如果有8个字段，就会分别执行8条sql，后来我又把粒度改为表级别：1234SELECT concat('select \'',TABLE_NAME,'\' as table_name, ','count(*) as num, ', group_concat(concat('count(', COLUMN_NAME, ')/count(*) as ',COLUMN_NAME)),' from ', TABLE_SCHEMA, '.', TABLE_NAME,';')FROM information_schema.COLUMNS GROUP BY TABLE_SCHEMA, TABLE_NAME; 结果如下图：&emsp;&emsp;这里用到了 group_concat 函数，concat大家多多少都知道，group_concat，看到group应该也理解的差不多了。这里要注意group_concat默认 连接字符串的长度就1024，而我们生成的sql有可能大于1024长度，所以这里要注意，改变这个长度：1SET GROUP_CONCAT_MAX_LEN = 10000000000; &emsp;&emsp;这个只对你当前会话生效，或者直接简单粗暴把global参数修改掉：1SET GLOBAL GROUP_CONCAT_MAX_LEN = 10000000000; &emsp;&emsp;好只剩最后一步了，批量生成好了sql,如何批量执行，并把结果存入文件里呢，直接上脚本：1234mysql -h#&#123;host&#125; -u#&#123;username&#125; -p#&#123;password&#125; -P#&#123;port&#125; -N -e "SET GROUP_CONCAT_MAX_LEN = 10000000000;SELECT concat('select \'',TABLE_NAME,'\' as table_name, ','count(*) as num, ', group_concat(concat('count(', COLUMN_NAME, ')/count(*) as ',COLUMN_NAME)),' from ', TABLE_SCHEMA, '.', TABLE_NAME,';')FROM information_schema.COLUMNSGROUP BY TABLE_SCHEMA, TABLE_NAME;"|mysql -h#&#123;host&#125; -u#&#123;username&#125; -p#&#123;password&#125; -P#&#123;port&#125; -f &gt;result.txt; 结果如图：之后把整个结果粘贴到excel里发送给产品，大功告成： &emsp;&emsp;解释下这条脚本的意思，”|” 符号前是执行sql， -e即执行mysql的sql语句,-f即忽略mysql的错误，继续向下执行。”|”后半句是，执行前面得到的sql,然后将结果输入到result.txt]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>group_concat</tag>
        <tag>mysql script</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[experience]]></title>
    <url>%2F2018%2F03%2F13%2Fexperince%2F</url>
    <content type="text"><![CDATA[收录一些牛客网或者其他网的优质面经，然后我根据自身情况，进行答案的填写。让我们时刻铭记面试套路，想去哪就去哪 &gt;_&lt; netEase music(路人A) 介绍一下你自己？ 我16年毕业于浙工大，然后去了一家做房地产互联网的B轮的公司——销冠科技。从16年年初呆到17年年末，期间经历了整个公司.net转java的技术转型，让我收益颇多。后来去了一家由一批资深阿里员工刚创业的金融公司叫杭州潘帕斯信息科技有限公司。相较于前者，这家公司给我的感觉就是氛围更加工程师化，所用的技术也更加的geek. 公司技术框架选型？ 主要就是spring全家桶,spring spring-boot spring-mvc等，然后orm框架用的是mybatis。在优化接口以及聚合数据的时候用过reids elasticsearch 等nosql。通过kafka实现缓存的更新，通过dubbo和别的系统进行rpc调用，通过camel进行数据的etl。 redis的数据结构 由5中数据结构，分别是 string, set,list zset,hash。和java里的类型很像。 redis的预热，雪崩，击穿,缓存算法 之所以使用redis主要是想优化接口的速度，一种是数据聚合接口，即app首页的一些信息，如总成交量，总客户数，认筹，认购，成交数之类。还有一种就是内部组织架构接口，由于开发商层级结构比较多，所以接口速度较慢 写了个main，直接跑一次，相当于把开发商的聚合接口都手动调用一遍，然后存入redis,失效时间是一个天。后续数据有改动，就在增改的接口上加个注解，然后通过kafka把增改的信息都记录下来，存入topic，通过key做hash让同一个开发商的消息放在一个partition里。然后消费的时候把不同partition里的数据取出来分别放入不同的阻塞队列，并且这个队列会去重，然后在调用对应接口去更新数，并重置失效时间 所谓击穿对我来说是不存在的，因为首页的接口，app只需要给我一个token，然后我根据token进行用户校验以及权限校验，如果乱传，则在拦截器里就被拦截掉了，所以不存在击穿。 雪崩的话，因为我这边的数据实时性要求较高，但是改动又不是很频繁，所以我这边的策略就是将缓存时间设为一个很大的值，然后再加个随机值，然后数据有所改变后，直接发消息到kafka队列，然后写个消费端去更新缓存。 如果真雪崩，那只能把请求加入队列，不能让mysql挂掉 FIFO算法:First in First out，先进先出。原则：一个数据最先进入缓存中，则应该最早淘汰掉。也就是说，当缓存满的时候，应当把最先进入缓存的数据给淘汰掉。 LFU算法：Least Frequently Used，最不经常使用算法。 LRU算法：Least Recently Used，近期最少使用算法 redis持久化 总共有两种方式，分别是 RDB(redis database)以及 AOF(append-only file). RDB即redis在n秒内如果有超过m个key被修改，则进行一次数据全量的snapshot，这个操作会单独开个子进程去操作，所以不会对主进程有任何影响。由于是全量，这个持久化的时间间隔不能太短，不然很消耗内存，因此如果中间发生故障，会丢失这个时间段的数据。 AOF即将”操作+数据”以格式化指令的方式存到文件中，可以理解成相当于 mysql的binlog。由于每次操作都是增量的，所以他可以时间间隔更短的录入。实时，每隔1秒等。如果数据修改操作很频繁，则这个日志文件会很大，所有AOF提供了rewrite机制，即保留当前的数据状态就好。rewrite机制不会重写原来文件，而是将redis里变更的数据， hashmap java7 和 java 8有啥区别 java7:使用一个Entry数组来存储数据，用key的hashcode取模来决定key会被放到数组里的位置，如果hashcode相同，或者hashcode取模后的结果相同（hash collision），那么这些key会被定位到Entry数组的同一个格子里，这些key会形成一个链表。在hashcode特别差的情况下，比方说所有key的hashcode都相同，这个链表可能会很长，那么put/get操作都可能需要遍历这个链表也就是说时间复杂度在最差情况下会退化到O(n) java8:使用一个Node数组来存储数据，但这个Node可能是链表结构，也可能是红黑树结构,如果插入的key的hashcode相同，那么这些key也会被定位到Node数组的同一个格子里。如果同一个格子里的key不超过8个，使用链表结构存储。如果超过了8个，那么会调用treeifyBin函数，将链表转换为红黑树。那么即使hashcode完全相同，由于红黑树的特点，查找某个特定元素，也只需要O(log n)的开销。也就是说put/get的操作的时间复杂度最差只有O(log n) j.u.c包下的新建线程池所用的类，为什么推荐用spring里的类 总共有四种线程池,这四种线程池都是通过ThreadPoolExecutor实现的 Executors.newCachedThreadPool()：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。（线程最大并发数不可控制） Executors.newFixedThreadPool()：创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 Executors.newScheduledThreadPool()：创建一个定长线程池，支持定时及周期性任务执行。 Executors.newSingleThreadExecutor()：创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 ThreadPoolExecutor中比较容易让人误解的是：corePoolSize，maximumPoolSize，workQueue之间关系 当线程池小于corePoolSize时，新提交任务将创建一个新线程执行任务，即使此时线程池中存在空闲线程。 当线程池达到corePoolSize时，新提交任务将被放入workQueue中，等待线程池中任务调度执行 当workQueue已满，且maximumPoolSize&gt;corePoolSize时，新提交任务会创建新线程执行任务 当提交任务数超过maximumPoolSize时，新提交任务由RejectedExecutionHandler处理 当线程池中超过corePoolSize线程，空闲时间达到keepAliveTime时，关闭空闲线程 当设置allowCoreThreadTimeOut(true)时，线程池中corePoolSize线程空闲时间达到keepAliveTime也将关闭 推荐用spring的 ThreadPoolTaskExecutor FixedThreadPool 和 SingleThreadPool:允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool 和 ScheduledThreadPool:允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 有过内存溢出调优经历吗 有过一次，再压测某个接口的时候，oom了，但我接口最后返回给前端的字段就是个位数啊，后来用jvisualvm工具看了一下，发现是rpc调用组织架构接口，返回了很多不必要的数据导致的。调用别人接口要传一个dto，如果dto里什么都不传，则把这个人所有的上级人员信息都返回，因此造成了内存溢出。 mysql聚簇索引，索引类型，哪种索引快，mysql所用数据结构。 mysql优化经验 说说对dubbo的了解 dubbo是一个rpc框架，它的整体架构是由5个节点组成的，分别是: provider:暴露服务的服务提供方 Consumer:调用远程服务的服务消费方 Registry:服务注册与发现的注册中心 Monitor:统计服务的调用次数和调用时间的监控中心 Container:服务运行容器 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 spring cloud 和 dubbo的区别 spring cloud 社区更活跃，而dubbo之前公司用的时候，阿里已经放弃维护了 调用方式 RPC|REST API ,这两种区别意味着，dubbo对服务提供方与调用房接口依赖方式太强了，需要引入jar,很容易产生循环依赖的问题。 Dubbo只实现了服务治理,而spring cloud覆盖了微服务架构下的方方面面 服务注册中心 Zookeeper multicast ,redis… | Eureka 服务跟踪 需自己集成 | spring cloud sleuth 分布式配置 无 | Spring cloud config kafka项目里如何保证数据的准确性 kafka的整个工作过程 spring boot有什么好处 最近做的项目介绍一下 jvm java 7 的和jvm 8 的有啥区别 说一下jvm jvm垃圾回收 说说elasticsearch的原理 倒排索引 http了解吗，说说http报文 https 和 http的区别 linux命令找出几分钟里面被修改的文件 找1-100里的质数 tcp三次握手和四次挥手 java8,java9新特性 设计模式 说说spring spring-boot,spring-mvc 算法题 说说AOP cglib asm mysql事务隔离级别 多线程，锁]]></content>
      <categories>
        <category>experience</category>
      </categories>
      <tags>
        <tag>experience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centOS6装mysql5.7]]></title>
    <url>%2F2018%2F03%2F08%2Fcentos6%E8%A3%85mysql5.7%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;去搬瓦工买了台vps,额只当作shadowSocks的server端，有点浪费。就打算把我爬虫用到的数据库直接装到服务器上。 &emsp;&emsp;本来想用docker…想想这超级低配主机还是别这么花里胡哨，老老实实传统方法安装好了。网上教程很多，我比较了一下，最终选了一篇内容看着比较详细靠谱也是centOS6 x86装mysql5.7的博文（下篇用到这篇文章的地方直接简称 博文），这里就简单记录一下安装过程中碰到的一些坑或者注意事项 centOS 选择 yum 安装rpm包是方便的方法之一,很多教程直接写死了 wget xxx某个版本的rpm包。其实应该根据操作系统选择最合适自己的包，官网 yum下载地址,比如我就是centos6所以我选 mysql5.7 默认需要复杂的密码(密码必须符合长度要求，且必须含有数字，小写或大写字母，特殊字符),如想想使用简单密码，则可以改变mysql全局变量 1mysql&gt; set global validate_password_policy=0; centos6 还未提供 systemctl,所以应该通过 1$ service mysqld start 启动好mysql 发现远程主机访问不了，多半是2个原因: 操作系统自身防火墙问题(自行google) mysql5.7默认是不允许远程主机访问的。解决方案]]></content>
      <categories>
        <category>瞎折腾</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[谈谈Elasticsearch聚合之如何group by与having]]></title>
    <url>%2F2018%2F03%2F06%2Felasticsearch1%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;最近工作中碰到了一些elasticsearch聚合的问题。明明sql都会写，但是数据导入es里之后，就不知道怎么写Query DSL取出来TmT,有木有。在这里记录一下,并整理出一个例子，供大家直接上手尝试。 开始扯淡0v0&emsp;&emsp;我们就从问题出发。首先起个elasticsearch 5.x(macOS推荐brew 或者 docker),保证localhost:9200能访问通。然后建立一个type，cURL：12345678910111213141516$ curl -X PUT \ http://localhost:9200/shirts \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: 360fcbcc-75ef-6dac-baec-3e5f02b9f3b8' \ -d '&#123; "mappings": &#123; "item": &#123; "properties": &#123; "brand": &#123; "type": "keyword"&#125;, "color": &#123; "type": "keyword"&#125;, "model": &#123; "type": "keyword"&#125; &#125; &#125; &#125; &#125;' 然后mock一些数据，cURL:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ curl -X PUT \ http://localhost:9200/shirts/item/1 \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: f767d0ad-a6cc-681c-36a1-90843594629e' \ -d '&#123; "brand": "gucci", "color": "red", "model": "big" &#125;'$ curl -X PUT \ http://localhost:9200/shirts/item/2 \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: f767d0ad-a6cc-681c-36a1-90843594629e' \ -d '&#123; "brand": "gucci", "color": "red", "model": "small" &#125;'$ curl -X PUT \ http://localhost:9200/shirts/item/3 \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: f767d0ad-a6cc-681c-36a1-90843594629e' \ -d '&#123; "brand": "nike", "color": "white", "model": "ugly" &#125;'$ curl -X PUT \ http://localhost:9200/shirts/item/4 \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: f767d0ad-a6cc-681c-36a1-90843594629e' \ -d '&#123; "brand": "nike", "color": "red", "model": "nice" &#125;'$ curl -X PUT \ http://localhost:9200/shirts/item/5 \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: f767d0ad-a6cc-681c-36a1-90843594629e' \ -d '&#123; "brand": "puma", "color": "yellow", "model": "honor" &#125;' &emsp;&emsp;brand是品牌，color是颜色,model是款式。然后brand,color,model三者确定数据唯一性，其他没任何限制，那么问题来了，只有一种颜色的品牌 总共有几个?下面是对应的sql:1234567891011121314151617CREATE TABLE `item` ( `id` int(11) NOT NULL AUTO_INCREMENT, `brand` varchar(20) DEFAULT NULL, `color` varchar(20) DEFAULT NULL, `model` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO item (id, brand, color, model) VALUES (1, 'gucci', 'red', 'big');INSERT INTO item (id, brand, color, model) VALUES (2, 'gucci', 'red', 'small');INSERT INTO item (id, brand, color, model) VALUES (3, 'nike', 'white', 'ugly');INSERT INTO item (id, brand, color, model) VALUES (4, 'nike', 'red', 'nice');INSERT INTO item (id, brand, color, model) VALUES (5, 'puma', 'yellow', 'honor');#1 返回只拥有一种颜色的品牌select cc.brand from (select brand,color from item GROUP BY brand, color)cc GROUP BY cc.brand HAVING count(cc.brand)=1#2 返回只拥有一种颜色的品牌 总共个数select count(1) FROM (select cc.brand from (select brand,color from item GROUP BY brand, color)cc GROUP BY cc.brand HAVING count(cc.brand)=1)dd; 上述结果分别为 gucci,puma; 2;那么es的 Query DSL如何写呢？直接上答案：123456789101112131415161718192021222324252627282930$ curl -X POST \ http://localhost:9200/shirts/_search \ -H 'Cache-Control: no-cache' \ -H 'Content-Type: application/json' \ -H 'Postman-Token: a95749ae-f92a-4f3d-0b7d-c0fec107cf5d' \ -d '&#123; "aggs": &#123; "groupby1": &#123; "terms": &#123; "field": "brand" &#125;, "aggs": &#123; "groupby2": &#123; "cardinality": &#123; "field": "color" &#125; &#125;, "sales_bucket_filter": &#123; "bucket_selector": &#123; "buckets_path": &#123; "groupby2": "groupby2" &#125;, "script": "params.groupby2 == 1" &#125; &#125; &#125; &#125; &#125; ,"size": 0&#125;' 返回的response为：123456789101112131415161718192021222324252627282930313233343536&#123; "took": 1, "timed_out": false, "_shards": &#123; "total": 5, "successful": 5, "failed": 0 &#125;, "hits": &#123; "total": 5, "max_score": 0, "hits": [] &#125;, "aggregations": &#123; "groupby1": &#123; "doc_count_error_upper_bound": 0, "sum_other_doc_count": 0, "buckets": [ &#123; "key": "gucci", "doc_count": 2, "groupby2": &#123; "value": 1 &#125; &#125;, &#123; "key": "puma", "doc_count": 1, "groupby2": &#123; "value": 1 &#125; &#125; ] &#125; &#125;&#125; &emsp;&emsp;最后只要取buckets.size就好了（有点间接0 0，网上实在是搜不到如何直接取count的那种了，有好方法，欢迎留言），大功告成。至于如何理解这个Query DSL，其实和sql很像，你可以对比我写的sql来理解。如果你是想搞清楚到底是咋回事的话，还是直接看官方文档吧，es文档写的比较清楚且好理解,下面是中英文文档的地址，哪里不懂看哪里即可： es的aggregations(聚合) 官方文档 中文文档 es的cardinality字段啥意思(即Cardinality Aggregation) 官方文档 中文文档 es的如何group by field1,field2(即Children Aggregation) 官方文档 中文文档 我上面使用的 sales_bucket_filter是什么意思(即Bucket Script Aggregation) 官方文档 中文文档 题外话&emsp;&emsp;在es5.x开始 string字段就被当作废弃字段，而取而代之的是text,keyword。text你可以近似的理解成2.x里的：123456789&#123;"mappings": &#123; "item": &#123; "properties": &#123; "test_word": &#123; "type": "string","index": "analyzed"&#125; &#125; &#125; &#125;&#125; 而keyword则理解成：123456789&#123;"mappings": &#123; "item": &#123; "properties": &#123; "test_word": &#123; "type": "string","index": "not_analyzed"&#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch Aggregations</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奇葩问题与神奇想法]]></title>
    <url>%2F2018%2F03%2F01%2F%E7%96%91%E9%97%AE%2F</url>
    <content type="text"><![CDATA[平时工作或者学习时，碰到的奇葩问题以及有时候自己坑自己的神奇想法 T-T，如果有人对某些东西有见解，可以在下面留言(梯子自备) 奇葩问题神奇想法 在mysql中，select id from tableA where id in (select id from tableB where 1=0)的结果并没有报错，并返回预期的结果，即什么都找不到。用过mybatis的人都知道，我们用foreach标签配in后面的参数的时候，都要特别注意list为空的情况，因为 where xxx in ()这样是会报错。那mysql内部是如何避免这种错误的呢0 0？]]></content>
  </entry>
  <entry>
    <title><![CDATA[JAVA知识点整理]]></title>
    <url>%2F2018%2F02%2F28%2F%E9%9B%86%E5%90%88%2F</url>
    <content type="text"><![CDATA[这里是阅读java8源码以及一些java知识的随笔，没头没尾，不严谨，仅自己看看 集合 List ArrayList 有3种构造函数。无参则elementData = {};有int参则负数报错，非负数初始化相应大小数组;Collection&lt;? extends E&gt; c入参的，直接elementData = c.toArray(); Integer.MAX_VALUE+1 返回的是最大的负数而不是最小的负数。所以c= Integer.MAX_VALUE+1 ,c= c-2,最终c为正数 负数向右移,不管移多少位始终都是负数 add和addAll方法会判断elementData这个数组容器是否为空，为空则初始化，如果是add则值为默认的10，如果是addAll方法，则选入参 size和10较大的一个值 add(E e)方法 如果未初始化，则初始化数组，默认长度为10 如果初始化过后，则每次扩容，扩容为 newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),即原来的1.5倍,如果为原来的1.5倍容量溢出了，则 如果上面一步扩容为原来的1.5倍超过Integer.MAX_VALUE-8了但又没溢出，则令容量等于Integer.MAX_VALUE，否则报错 Arrays.copyOf 是浅拷贝，浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。如果属性是基本类型，拷贝的就是基本类型的值，如果属性是内存地址（引用类型），拷贝的就是内存地址 add(int index, E element) 检查index是否比 size大，比0小 之后就是容量检测，扩容,溢出检测等，和上同 之后调用System.arraycopy(elementData, index, elementData, index + 1,size - index)。 最后elementData[index] = element; size++; get(int index) 判断范围，越界报错 return elementData[index] set(int index, E element) 在 index位置上放入新值，返回旧值 remove(int index) 删除index上的值，调用System.arraycopy，return 删掉的值 返回的是删掉的值 remove(Object o) 循环，遍历，找到一个，就删掉，并调用System.arraycopy 返回的是bool removeAll(Collection&lt;?&gt; c) for循环 判断 c里面是否有list里的元素c.contains(elementData[r])==false,如果没有，则 elementData[w++] = elementData[r]。 如果抛异常了，则将后面还没遍历到的数据放到从索引w处开始的地方 subList(int fromIndex, int toIndex) return new SubList(this, 0, fromIndex, toIndex) 从 this，可以看出生成的只是当前list的截取视图 trimToSize 去掉扩容多于的部分，使数组长度好等于size contains(Object o ) 循环遍历数组 找到equals的返回true LinkedList add(int index, E element) 校验 index不能想超过size 如果list里已经有一个元素2，如果 add(0,1),则元素list数据为[1,2],如果add(1,2),则结果为[2,1],即可决定加在链表头还是尾，或者是链表任意一个位置 add(E e)方法 校验 在链表尾加一个node addAll(int index, Collection&lt;? extends E&gt; c) 会将 collection类型的数据先变成一个链表，再插入原先链表的任意位置，位置随index而定，与add方法类似 node(int index) 首先会判断 index &lt; (size &gt;&gt; 1) 是否成立，来判断从头开始找还是从尾巴开始找，所以随机读取效率低 get(int index) 检验 调用node(int index) remove() 删除第一个 set(int index, E element) 直接通过node(i)方法找到该node,然后直接替换该node.item 2种构造函数。无参构造函数，就是个空方法0 0，有参构造函数调用addAll方法， 疑问 System.arraycopy(elementData, index, elementData, index + 1,size - index)，数组自己拷贝到自己时，明明有重叠部分，却并不会覆盖？由于是native方法，猜测有可能是从数组尾部开始复制的！ list 里有个 modcount有啥用？ 后来发现有些方法里有判断 modCount == expectedModCount，所以modCount是用来保证，多线程操作时，抛出错误，阻止方法执行]]></content>
      <categories>
        <category>JAVA知识点</category>
      </categories>
      <tags>
        <tag>JAVA知识点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIFI-安装]]></title>
    <url>%2F2018%2F02%2F27%2Fnifi-1%2F</url>
    <content type="text"><![CDATA[NIFI网上的中文资料是真的少。特别是用docker启动NIFI。记录下来，当传家宝=v = 安装NIFImacOS没有 brew 的先安装brew1$ brew install nifi 通过brew安装完之后12$ cd /usr/local/Cellar/nifi/1.5.0/bin$ sudo nifi start 注意，上述路径有可能和作者不一样。因为对应的nifi版本不一样！！！ Linux官网下载,下完之后找到工程目录 bin 然后 1$ sudo nifi start 附加牢骚(我懒得试0v0,就试了用docker启动ubuntu16.04,然后不行)： Ubuntu / Debian 系的可以试试看： 1$ sudo apt-get install nifi Centos / Redhat 可以试试: 1$ sudo yum install nifi 以上安装方法默认 java环境已经安装好 并且是jdk1.8或以上。 docker容器大爱，只需要有docker环境就好了,非常方便，推荐使用docker安装12$ docker pull apache/nifi:1.5.0$ docker run --name nifi -p 8080:8080 -d apache/nifi:1.5.0 启动成功会显示该新建容器的id(这个id每个人都是不一样的):1$ c78fabb5d1eb99be5de736499f3c833e6e7a6d0608ef4457ee348887e22a35ce 关闭容器后，想第二次启动该容器,不应该重复上述的docker run命令，否则会报如下错误:123$ docker run --name nifi -p 8080:8080 -d apache/nifi:1.5.0$ docker: Error response from daemon: Conflict. The container name "/nifi" is already in use by container "c78fabb5d1eb99be5de736499f3c833e6e7a6d0608ef4457ee348887e22a35ce". You have to remove (or rename) that container to be able to reuse that name. See 'docker run --help'. 应根据名字或者id来启动，名称即 –name 后面的参数，这里是nifi，所以命令如下:1$ docker start nifi 启动 docker 容器命令后面也可以加个 –rm,即1$ docker run --rm --name nifi -p 8080:8080 -d apache/nifi:1.5.0 这个参数是说容器退出后随之将其删除。这样你每次起的都是一个全新的镜像。 搞定收工]]></content>
      <categories>
        <category>NIFI</category>
      </categories>
      <tags>
        <tag>NIFI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Next折腾记录]]></title>
    <url>%2F2018%2F02%2F26%2Fhexo-next-1%2F</url>
    <content type="text"><![CDATA[本文是我折腾Hexo Next主题的一些心得，有些坑，真的是不踩不知道，一踩要踩一天TmT。这里我总结一下，让后来人站在我这个巨人的肩膀上 =v =嘎嘎。 &emsp;&emsp; 首先，如果对Hexo一无所知，不知道是个什么东西的，出门左转 -&gt; Hexo 官方文档。我稍微解释一下Hexo到底是什么吧，网上好多文档都没说它到底有个卵用，就开始长篇大论了，这让我搞了好久都不知道Hexo到底有啥用，我为啥要用。Hexo其实可以理解为一个Html生成器，给不会前端(html)或者想快速搭建自己博客的人提供。你只需在Hexo提供的工程目录下的配置文件里配置一些参数，以及通过markdown语法写下自己的博文，Hexo就帮你负责生成相应的Html和js，这样你就可以只关心自己博客该如何写，而不用考虑其他琐事(如何 html怎么写，css怎么写之类的)。&emsp;&emsp;那你可能又问为啥不用csdn 或者掘金之类的网站提供的博客功能。我的回答就是因为这样显得很装B，哈哈，说好听点，就是感觉比较 geek！额，扯远了，当你看完Hexo的官方文档，你对Hexo有个大概了解，以及如何基本操作后，你可以看看Next主题，出门右转 -&gt; Next 官方文档。&emsp;&emsp;废话讲完，进入踩坑细节。接下来的内容是建立在Hexo文档以及Next文档都已经了解的情况下写的。 那些天坑 Hexo 的 _config.xml里的 url:配置你的域名时，一定要加上http://或者https://,比如url: http://yewennuan.com。不加的话，如果你添加了生成站点地图sitemap.xml的功能，你会发现Google的Search Console收录你的站点地图也就是sitemap时，会报错，说sitemap地址格式错误T T。 如果你用 github pages托管Hexo生成的静态站点，那么baidu的爬虫是爬不到内容的，因为github 不让百度的爬虫爬取0-0(这不是坑爹吗，我去)，解决办法就是买个域名，如yewennuan.com，百度爬虫访问该域名解析到国内的静态资源托管网站 如Coding.net,你可以理解为中国的github，然后他也有pages服务，这个coding的pages服务如何搞可以参考Google搜索。具体DNS解析如何操作，请看下面图片:这是阿里云的云解析 DNS控制台,进行如下设置。(由于百度爬虫存在DNS缓存，所以这个方案还没得到验证，等验证完，我在过来更新！) 如果你的Next主题集成 disqus评论插件，发现怎么搞也出现不了。然后，你再看看这片文章结尾有没有评论栏，如果没有，额，兄弟，不用找了，墙里面是看不见的。你需要一个梯子╮(╯▽╰)╭ 如果你在万网购买了一个域名，然后你想解析到你的xxx.github.io。你只需在万网控制台(被阿里云收购了，也就是阿里云的云解析DNS控制台)进行如下操作：你可能又会问A记录为啥为啥指向这两个ip啊,可以见下图:,人家github pages的帮助文档就叫你指向这两个ip。然后你又会问CNAME记录为啥还要在解析一遍到xxx.github.io上啊，没有这个CNAME记录的配置，你会发现在chrome输入xxx.com如yewennuan.com也能访问到自己的博客了。其实增加CNAME记录是为了解析www域名前缀，你会发现没有配置CNAME,www.xxx.com如www.yewennuan.com是访问不通的，配置了就ok了！(这边只是讲了解析的一部分，还要在自己工程里配置CNAME文件以及github上配置域名等等就没赘述，自己Google或者官方文档看看)&emsp;&emsp;额，当时网上找了好久，我就是不知道为啥域名要解析到这两个ip，，网上一些教学文档也不说一下出处，还有些说ping一下 xxx.github.io，得到ip，然后将域名解析上去，尼玛。 暂时没了，以后被坑了继续补充]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
